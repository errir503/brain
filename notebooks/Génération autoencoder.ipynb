{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEwCAYAAAB2YUwcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCCGtYRIhhBDSmv8WMtliUWOsdgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b2f78d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import load_model\n",
    "from keras import *\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "datas = pd.read_csv(\"21-02-2018_18-31-45.csv\")\n",
    "datas = datas.drop([\"n\", \"time\"], axis=1)\n",
    "datas = (datas - datas.mean()) / datas.std()\n",
    "corr = datas.corr()\n",
    "sns.heatmap(corr)\n",
    "\n",
    "\n",
    "dataset = pd.read_csv(\"data15min.csv\")\n",
    "\n",
    "X = Y = dataset.sample(frac=0.8)\n",
    "X_test = Y_test = dataset.drop(X.index)\n",
    "activation = 'linear'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_335 (Dense)            (None, 50)                1300      \n",
      "_________________________________________________________________\n",
      "dense_336 (Dense)            (None, 15)                765       \n",
      "_________________________________________________________________\n",
      "dense_337 (Dense)            (None, 5)                 80        \n",
      "_________________________________________________________________\n",
      "dense_338 (Dense)            (None, 15)                90        \n",
      "_________________________________________________________________\n",
      "dense_339 (Dense)            (None, 50)                800       \n",
      "_________________________________________________________________\n",
      "dense_340 (Dense)            (None, 25)                1275      \n",
      "=================================================================\n",
      "Total params: 4,310\n",
      "Trainable params: 4,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3886 samples, validate on 180 samples\n",
      "Epoch 1/300\n",
      "3886/3886 [==============================] - 3s 649us/step - loss: 674131.0455 - acc: 0.2885 - val_loss: 35757.1641 - val_acc: 0.4278\n",
      "Epoch 2/300\n",
      "3886/3886 [==============================] - 1s 258us/step - loss: 275265.4109 - acc: 0.4611 - val_loss: 24744.6332 - val_acc: 0.4500\n",
      "Epoch 3/300\n",
      "3886/3886 [==============================] - 1s 247us/step - loss: 119397.2091 - acc: 0.4738 - val_loss: 19848.5385 - val_acc: 0.4611\n",
      "Epoch 4/300\n",
      "3886/3886 [==============================] - 1s 251us/step - loss: 123684.6847 - acc: 0.4779 - val_loss: 25744.7949 - val_acc: 0.4444\n",
      "Epoch 5/300\n",
      "3886/3886 [==============================] - 1s 250us/step - loss: 31937.4787 - acc: 0.4735 - val_loss: 9590.3188 - val_acc: 0.4611\n",
      "Epoch 6/300\n",
      "3886/3886 [==============================] - 1s 249us/step - loss: 17191.4594 - acc: 0.4825 - val_loss: 9916.7204 - val_acc: 0.4389\n",
      "Epoch 7/300\n",
      "3886/3886 [==============================] - 1s 240us/step - loss: 42072.4687 - acc: 0.4936 - val_loss: 50422.8874 - val_acc: 0.4333\n",
      "Epoch 8/300\n",
      "3886/3886 [==============================] - 1s 236us/step - loss: 298952.9499 - acc: 0.4539 - val_loss: 22208.2637 - val_acc: 0.4444\n",
      "Epoch 9/300\n",
      "3886/3886 [==============================] - 1s 243us/step - loss: 105749.9377 - acc: 0.5093 - val_loss: 19382.8329 - val_acc: 0.4611\n",
      "Epoch 10/300\n",
      "3886/3886 [==============================] - 1s 249us/step - loss: 105035.4197 - acc: 0.5080 - val_loss: 11094.6145 - val_acc: 0.4778\n",
      "Epoch 11/300\n",
      "3886/3886 [==============================] - 1s 260us/step - loss: 17874.8362 - acc: 0.5319 - val_loss: 9068.8097 - val_acc: 0.5444\n",
      "Epoch 12/300\n",
      "3886/3886 [==============================] - 1s 259us/step - loss: 31460.6962 - acc: 0.5445 - val_loss: 13879.6068 - val_acc: 0.5222\n",
      "Epoch 13/300\n",
      "3886/3886 [==============================] - 1s 254us/step - loss: 405858.6021 - acc: 0.5993 - val_loss: 33321.4079 - val_acc: 0.6556\n",
      "Epoch 14/300\n",
      "3886/3886 [==============================] - 1s 250us/step - loss: 278188.2558 - acc: 0.7072 - val_loss: 49397.0782 - val_acc: 0.6444\n",
      "Epoch 15/300\n",
      "3886/3886 [==============================] - 1s 257us/step - loss: 122559.5754 - acc: 0.7355 - val_loss: 12201.9014 - val_acc: 0.7111\n",
      "Epoch 16/300\n",
      "3886/3886 [==============================] - 1s 263us/step - loss: 11431.2522 - acc: 0.7396 - val_loss: 15258.3347 - val_acc: 0.6778\n",
      "Epoch 17/300\n",
      "3886/3886 [==============================] - 1s 260us/step - loss: 14537.2084 - acc: 0.7303 - val_loss: 13645.3067 - val_acc: 0.7111\n",
      "Epoch 18/300\n",
      "3886/3886 [==============================] - 1s 256us/step - loss: 12267.2753 - acc: 0.7365 - val_loss: 13187.9489 - val_acc: 0.7111\n",
      "Epoch 19/300\n",
      "3886/3886 [==============================] - 1s 275us/step - loss: 11564.3837 - acc: 0.7393 - val_loss: 10248.7786 - val_acc: 0.7111\n",
      "Epoch 20/300\n",
      "3886/3886 [==============================] - 1s 262us/step - loss: 17148.3859 - acc: 0.7355 - val_loss: 13505.6273 - val_acc: 0.7056\n",
      "Epoch 21/300\n",
      "3886/3886 [==============================] - 1s 249us/step - loss: 363649.1049 - acc: 0.7298 - val_loss: 37533.2335 - val_acc: 0.7278\n",
      "Epoch 22/300\n",
      "3886/3886 [==============================] - 1s 259us/step - loss: 53040.7611 - acc: 0.7360 - val_loss: 20165.4342 - val_acc: 0.7333\n",
      "Epoch 23/300\n",
      "3886/3886 [==============================] - 1s 258us/step - loss: 50277.2385 - acc: 0.7535 - val_loss: 16916.0736 - val_acc: 0.7333\n",
      "Epoch 24/300\n",
      "3886/3886 [==============================] - 1s 263us/step - loss: 24372.0405 - acc: 0.7519 - val_loss: 12788.2871 - val_acc: 0.7389\n",
      "Epoch 25/300\n",
      "3886/3886 [==============================] - 1s 258us/step - loss: 48483.9635 - acc: 0.7589 - val_loss: 15933.9611 - val_acc: 0.7333\n",
      "Epoch 26/300\n",
      "3886/3886 [==============================] - 1s 262us/step - loss: 184528.2147 - acc: 0.7355 - val_loss: 56496.6384 - val_acc: 0.7278\n",
      "Epoch 27/300\n",
      "3886/3886 [==============================] - 1s 260us/step - loss: 198935.6205 - acc: 0.7419 - val_loss: 15070.7583 - val_acc: 0.7000\n",
      "Epoch 28/300\n",
      "3886/3886 [==============================] - 1s 256us/step - loss: 12358.9936 - acc: 0.7568 - val_loss: 11333.1616 - val_acc: 0.7278\n",
      "Epoch 29/300\n",
      "3886/3886 [==============================] - 1s 262us/step - loss: 31916.9646 - acc: 0.7445 - val_loss: 12655.3350 - val_acc: 0.7556\n",
      "Epoch 30/300\n",
      "3886/3886 [==============================] - 1s 263us/step - loss: 191599.5646 - acc: 0.7463 - val_loss: 15535.1981 - val_acc: 0.7444\n",
      "Epoch 31/300\n",
      "3886/3886 [==============================] - 1s 257us/step - loss: 126658.1918 - acc: 0.7434 - val_loss: 11759.4341 - val_acc: 0.7500\n",
      "Epoch 32/300\n",
      "3886/3886 [==============================] - 1s 258us/step - loss: 63946.7812 - acc: 0.7540 - val_loss: 7589.7330 - val_acc: 0.7389\n",
      "Epoch 33/300\n",
      "3886/3886 [==============================] - 1s 260us/step - loss: 28961.8417 - acc: 0.7604 - val_loss: 7346.6489 - val_acc: 0.7556\n",
      "Epoch 34/300\n",
      "3886/3886 [==============================] - 1s 254us/step - loss: 7594.7169 - acc: 0.7669 - val_loss: 7673.3108 - val_acc: 0.7667\n",
      "Epoch 35/300\n",
      "3886/3886 [==============================] - 1s 261us/step - loss: 8501.1322 - acc: 0.7620 - val_loss: 5079.8806 - val_acc: 0.7500\n",
      "Epoch 36/300\n",
      "3886/3886 [==============================] - 1s 252us/step - loss: 82337.8859 - acc: 0.7558 - val_loss: 18628.4962 - val_acc: 0.7222\n",
      "Epoch 37/300\n",
      "3886/3886 [==============================] - 1s 261us/step - loss: 214699.8842 - acc: 0.7486 - val_loss: 7457.5272 - val_acc: 0.7278\n",
      "Epoch 38/300\n",
      "3886/3886 [==============================] - 1s 265us/step - loss: 35220.6314 - acc: 0.7689 - val_loss: 5348.6422 - val_acc: 0.7500\n",
      "Epoch 39/300\n",
      "3886/3886 [==============================] - 1s 272us/step - loss: 8448.9159 - acc: 0.7766 - val_loss: 5745.1890 - val_acc: 0.7833\n",
      "Epoch 40/300\n",
      "3886/3886 [==============================] - 1s 281us/step - loss: 18119.6422 - acc: 0.7735 - val_loss: 3075.2320 - val_acc: 0.7111\n",
      "Epoch 41/300\n",
      "3886/3886 [==============================] - 1s 263us/step - loss: 38105.2863 - acc: 0.7560 - val_loss: 19788.8789 - val_acc: 0.7111\n",
      "Epoch 42/300\n",
      "3886/3886 [==============================] - 1s 259us/step - loss: 236628.3757 - acc: 0.7244 - val_loss: 15125.8839 - val_acc: 0.7500\n",
      "Epoch 43/300\n",
      "3886/3886 [==============================] - 1s 263us/step - loss: 41782.9680 - acc: 0.7741 - val_loss: 3346.1514 - val_acc: 0.7500\n",
      "Epoch 44/300\n",
      "3886/3886 [==============================] - 1s 264us/step - loss: 7226.1432 - acc: 0.7707 - val_loss: 1979.1128 - val_acc: 0.7444\n",
      "Epoch 45/300\n",
      "3886/3886 [==============================] - 1s 266us/step - loss: 8007.2471 - acc: 0.7800 - val_loss: 4605.4984 - val_acc: 0.7556\n",
      "Epoch 46/300\n",
      "3886/3886 [==============================] - 1s 256us/step - loss: 47094.1390 - acc: 0.7898 - val_loss: 4927.4622 - val_acc: 0.7778\n",
      "Epoch 47/300\n",
      "3886/3886 [==============================] - 1s 249us/step - loss: 25020.9088 - acc: 0.7756 - val_loss: 6752.3629 - val_acc: 0.7444\n",
      "Epoch 48/300\n",
      "3886/3886 [==============================] - 1s 261us/step - loss: 126167.5377 - acc: 0.7728 - val_loss: 38227.4407 - val_acc: 0.7500\n",
      "Epoch 49/300\n",
      "3886/3886 [==============================] - 1s 248us/step - loss: 114488.0421 - acc: 0.7563 - val_loss: 11313.7798 - val_acc: 0.6833\n",
      "Epoch 50/300\n",
      "3886/3886 [==============================] - 1s 242us/step - loss: 148937.8006 - acc: 0.7210 - val_loss: 9538.2055 - val_acc: 0.7222\n",
      "Epoch 51/300\n",
      "3886/3886 [==============================] - 1s 246us/step - loss: 20911.0169 - acc: 0.7599 - val_loss: 4876.0198 - val_acc: 0.7389\n",
      "Epoch 52/300\n",
      "3886/3886 [==============================] - 1s 241us/step - loss: 9250.6825 - acc: 0.7705 - val_loss: 5064.9423 - val_acc: 0.7333\n",
      "Epoch 53/300\n",
      "3886/3886 [==============================] - 1s 245us/step - loss: 24468.1984 - acc: 0.7648 - val_loss: 6883.2835 - val_acc: 0.7000\n",
      "Epoch 54/300\n",
      "3886/3886 [==============================] - 1s 252us/step - loss: 238181.9235 - acc: 0.7254 - val_loss: 6935.6180 - val_acc: 0.7611\n",
      "Epoch 55/300\n",
      "3886/3886 [==============================] - 1s 249us/step - loss: 84937.3801 - acc: 0.7283 - val_loss: 4001.1539 - val_acc: 0.7222\n",
      "Epoch 56/300\n",
      "3886/3886 [==============================] - 1s 251us/step - loss: 13703.9788 - acc: 0.7476 - val_loss: 8860.0835 - val_acc: 0.7278\n",
      "Epoch 57/300\n",
      "3886/3886 [==============================] - 1s 253us/step - loss: 42805.3390 - acc: 0.7607 - val_loss: 5456.0595 - val_acc: 0.7111\n",
      "Epoch 58/300\n",
      "3886/3886 [==============================] - 1s 264us/step - loss: 10596.6202 - acc: 0.7748 - val_loss: 2819.7220 - val_acc: 0.7333\n",
      "Epoch 59/300\n",
      "3886/3886 [==============================] - 1s 253us/step - loss: 80720.3391 - acc: 0.7620 - val_loss: 30696.0882 - val_acc: 0.7167\n",
      "Epoch 60/300\n",
      "3886/3886 [==============================] - 1s 259us/step - loss: 279105.2687 - acc: 0.7151 - val_loss: 13944.7443 - val_acc: 0.7000\n",
      "Epoch 61/300\n",
      "3886/3886 [==============================] - 1s 250us/step - loss: 16826.1641 - acc: 0.7437 - val_loss: 6738.0766 - val_acc: 0.7000\n",
      "Epoch 62/300\n",
      "3886/3886 [==============================] - 1s 265us/step - loss: 12986.8861 - acc: 0.7519 - val_loss: 7705.2361 - val_acc: 0.7111\n",
      "Epoch 63/300\n",
      "3886/3886 [==============================] - 1s 263us/step - loss: 10211.1400 - acc: 0.7519 - val_loss: 4782.3019 - val_acc: 0.7000\n",
      "Epoch 64/300\n",
      "3886/3886 [==============================] - 1s 258us/step - loss: 87474.1990 - acc: 0.7470 - val_loss: 3418.6222 - val_acc: 0.7000\n",
      "Epoch 65/300\n",
      "3886/3886 [==============================] - 1s 262us/step - loss: 68812.7475 - acc: 0.7504 - val_loss: 2927.4700 - val_acc: 0.7000\n",
      "Epoch 66/300\n",
      "3886/3886 [==============================] - 1s 252us/step - loss: 121766.9744 - acc: 0.7424 - val_loss: 2620.8561 - val_acc: 0.7056\n",
      "Epoch 67/300\n",
      "3886/3886 [==============================] - 1s 255us/step - loss: 8307.8382 - acc: 0.7576 - val_loss: 2120.9405 - val_acc: 0.7167\n",
      "Epoch 68/300\n",
      "3886/3886 [==============================] - 1s 260us/step - loss: 4674.9112 - acc: 0.7635 - val_loss: 2146.8411 - val_acc: 0.7278\n",
      "Epoch 69/300\n",
      "3886/3886 [==============================] - 1s 257us/step - loss: 11199.3628 - acc: 0.7651 - val_loss: 4282.4245 - val_acc: 0.7278\n",
      "Epoch 70/300\n",
      "3886/3886 [==============================] - 1s 257us/step - loss: 169514.9735 - acc: 0.7578 - val_loss: 7604.8905 - val_acc: 0.7167\n",
      "Epoch 71/300\n",
      "3886/3886 [==============================] - 1s 265us/step - loss: 33368.0318 - acc: 0.7653 - val_loss: 2910.1181 - val_acc: 0.7222\n",
      "Epoch 72/300\n",
      "3886/3886 [==============================] - 1s 280us/step - loss: 151454.0863 - acc: 0.7578 - val_loss: 8238.1724 - val_acc: 0.7222\n",
      "Epoch 73/300\n",
      "3886/3886 [==============================] - 1s 270us/step - loss: 154970.5447 - acc: 0.7347 - val_loss: 4624.0728 - val_acc: 0.6833\n",
      "Epoch 74/300\n",
      "3886/3886 [==============================] - 1s 254us/step - loss: 6785.9526 - acc: 0.7604 - val_loss: 2685.1029 - val_acc: 0.7056\n",
      "Epoch 75/300\n",
      "3886/3886 [==============================] - 1s 240us/step - loss: 5474.7955 - acc: 0.7715 - val_loss: 2230.7191 - val_acc: 0.7444\n",
      "Epoch 76/300\n",
      "3886/3886 [==============================] - 1s 221us/step - loss: 10601.1119 - acc: 0.7679 - val_loss: 2028.2380 - val_acc: 0.7222\n",
      "Epoch 77/300\n",
      "3886/3886 [==============================] - 1s 247us/step - loss: 15481.6297 - acc: 0.7710 - val_loss: 3835.6340 - val_acc: 0.7333\n",
      "Epoch 78/300\n",
      "3886/3886 [==============================] - 1s 255us/step - loss: 53708.8748 - acc: 0.7663 - val_loss: 5395.3755 - val_acc: 0.6889\n",
      "Epoch 79/300\n",
      "3886/3886 [==============================] - 1s 217us/step - loss: 325743.9297 - acc: 0.7344 - val_loss: 3267.4908 - val_acc: 0.7000\n",
      "Epoch 80/300\n",
      "3886/3886 [==============================] - 1s 209us/step - loss: 9572.2636 - acc: 0.7578 - val_loss: 6229.9450 - val_acc: 0.7222\n",
      "Epoch 81/300\n",
      "3886/3886 [==============================] - 1s 211us/step - loss: 17491.1685 - acc: 0.7674 - val_loss: 2491.6194 - val_acc: 0.7278\n",
      "Epoch 82/300\n",
      "3886/3886 [==============================] - 1s 218us/step - loss: 6449.7365 - acc: 0.7653 - val_loss: 1815.5863 - val_acc: 0.7278\n",
      "Epoch 83/300\n",
      "3886/3886 [==============================] - 1s 213us/step - loss: 13179.8549 - acc: 0.7674 - val_loss: 8273.2291 - val_acc: 0.7167\n",
      "Epoch 84/300\n",
      "3886/3886 [==============================] - 1s 213us/step - loss: 18286.9170 - acc: 0.7725 - val_loss: 5647.4442 - val_acc: 0.7444\n",
      "Epoch 85/300\n",
      "3886/3886 [==============================] - 1s 213us/step - loss: 51180.4604 - acc: 0.7743 - val_loss: 3209.0621 - val_acc: 0.7389\n",
      "Epoch 86/300\n",
      "3886/3886 [==============================] - 1s 213us/step - loss: 7931.6989 - acc: 0.7818 - val_loss: 7034.5439 - val_acc: 0.7389\n",
      "Epoch 87/300\n",
      "3886/3886 [==============================] - 1s 221us/step - loss: 4624.0202 - acc: 0.7790 - val_loss: 1930.2662 - val_acc: 0.7444\n",
      "Epoch 88/300\n",
      "3886/3886 [==============================] - 1s 215us/step - loss: 37481.1261 - acc: 0.7710 - val_loss: 11359.0637 - val_acc: 0.7222\n",
      "Epoch 89/300\n",
      "3886/3886 [==============================] - 1s 212us/step - loss: 48658.5306 - acc: 0.7666 - val_loss: 11142.8683 - val_acc: 0.7167\n",
      "Epoch 90/300\n",
      "3886/3886 [==============================] - 1s 210us/step - loss: 155573.8832 - acc: 0.7409 - val_loss: 5806.7930 - val_acc: 0.7056\n",
      "Epoch 91/300\n",
      "3886/3886 [==============================] - 1s 211us/step - loss: 13379.4178 - acc: 0.7612 - val_loss: 4493.3153 - val_acc: 0.7167\n",
      "Epoch 92/300\n",
      "3886/3886 [==============================] - 1s 210us/step - loss: 6332.4856 - acc: 0.7702 - val_loss: 1228.0646 - val_acc: 0.7111\n",
      "Epoch 93/300\n",
      "3886/3886 [==============================] - 1s 207us/step - loss: 29314.9085 - acc: 0.7795 - val_loss: 108978.6771 - val_acc: 0.7278\n",
      "Epoch 94/300\n",
      "3886/3886 [==============================] - 1s 210us/step - loss: 316551.2098 - acc: 0.7578 - val_loss: 30549.6755 - val_acc: 0.6944\n",
      "Epoch 95/300\n",
      "3886/3886 [==============================] - 1s 266us/step - loss: 72939.8038 - acc: 0.7594 - val_loss: 12567.8762 - val_acc: 0.7111\n",
      "Epoch 96/300\n",
      "3886/3886 [==============================] - 1s 250us/step - loss: 83116.2054 - acc: 0.7712 - val_loss: 4450.7877 - val_acc: 0.7278\n",
      "Epoch 97/300\n",
      "3886/3886 [==============================] - 1s 249us/step - loss: 9522.3261 - acc: 0.7751 - val_loss: 3565.6044 - val_acc: 0.7278\n",
      "Epoch 98/300\n",
      "3886/3886 [==============================] - 1s 252us/step - loss: 25967.5654 - acc: 0.7663 - val_loss: 7816.2301 - val_acc: 0.7222\n",
      "Epoch 99/300\n",
      "3886/3886 [==============================] - 1s 253us/step - loss: 20092.9634 - acc: 0.7576 - val_loss: 2057.6766 - val_acc: 0.7222\n",
      "Epoch 100/300\n",
      "3886/3886 [==============================] - 1s 252us/step - loss: 4452.8962 - acc: 0.7743 - val_loss: 3751.7937 - val_acc: 0.7222\n",
      "Epoch 101/300\n",
      "3886/3886 [==============================] - 1s 250us/step - loss: 73582.8670 - acc: 0.7550 - val_loss: 2953.7505 - val_acc: 0.7167\n",
      "Epoch 102/300\n",
      "3886/3886 [==============================] - 1s 258us/step - loss: 35005.9714 - acc: 0.7730 - val_loss: 3106.7591 - val_acc: 0.7389\n",
      "Epoch 103/300\n",
      "3886/3886 [==============================] - 1s 269us/step - loss: 58140.4502 - acc: 0.7753 - val_loss: 2558.8145 - val_acc: 0.7222\n",
      "Epoch 104/300\n",
      "3886/3886 [==============================] - 1s 265us/step - loss: 10719.5139 - acc: 0.7707 - val_loss: 2117.7719 - val_acc: 0.7278\n",
      "Epoch 105/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3886/3886 [==============================] - 1s 278us/step - loss: 77151.4898 - acc: 0.7656 - val_loss: 1699.8526 - val_acc: 0.7278\n",
      "Epoch 106/300\n",
      "3886/3886 [==============================] - 1s 246us/step - loss: 46428.4069 - acc: 0.7723 - val_loss: 1649.6161 - val_acc: 0.7111\n",
      "Epoch 107/300\n",
      "3886/3886 [==============================] - 1s 232us/step - loss: 3101.8482 - acc: 0.7735 - val_loss: 1868.7279 - val_acc: 0.7167\n",
      "Epoch 108/300\n",
      "3886/3886 [==============================] - 1s 231us/step - loss: 13418.8289 - acc: 0.7656 - val_loss: 1914.1063 - val_acc: 0.7167\n",
      "Epoch 109/300\n",
      "3886/3886 [==============================] - 1s 234us/step - loss: 12190.7036 - acc: 0.7607 - val_loss: 1728.0931 - val_acc: 0.7222\n",
      "Epoch 110/300\n",
      "3886/3886 [==============================] - 1s 240us/step - loss: 3918.6798 - acc: 0.7694 - val_loss: 1343.0292 - val_acc: 0.7389\n",
      "Epoch 111/300\n",
      "3886/3886 [==============================] - 1s 231us/step - loss: 50217.6959 - acc: 0.7630 - val_loss: 43919.1964 - val_acc: 0.7389\n",
      "Epoch 112/300\n",
      "3886/3886 [==============================] - 1s 239us/step - loss: 93521.0324 - acc: 0.7661 - val_loss: 17153.4247 - val_acc: 0.7111\n",
      "Epoch 113/300\n",
      "3886/3886 [==============================] - 1s 240us/step - loss: 251518.8698 - acc: 0.7573 - val_loss: 26255.8643 - val_acc: 0.7389\n",
      "Epoch 114/300\n",
      "3886/3886 [==============================] - 1s 251us/step - loss: 123291.7796 - acc: 0.7383 - val_loss: 16348.3069 - val_acc: 0.7389\n",
      "Epoch 115/300\n",
      "3886/3886 [==============================] - 1s 250us/step - loss: 210527.7173 - acc: 0.7645 - val_loss: 4472.5022 - val_acc: 0.7278\n",
      "Epoch 116/300\n",
      "3886/3886 [==============================] - 1s 237us/step - loss: 29712.5526 - acc: 0.7705 - val_loss: 3268.4935 - val_acc: 0.7333\n",
      "Epoch 117/300\n",
      "3886/3886 [==============================] - 1s 262us/step - loss: 15370.0446 - acc: 0.7797 - val_loss: 2011.5083 - val_acc: 0.7500\n",
      "Epoch 118/300\n",
      "3886/3886 [==============================] - 1s 249us/step - loss: 23209.4266 - acc: 0.7769 - val_loss: 4630.7341 - val_acc: 0.7389\n",
      "Epoch 119/300\n",
      "3886/3886 [==============================] - 1s 263us/step - loss: 3643.1069 - acc: 0.7697 - val_loss: 2152.6067 - val_acc: 0.7333\n",
      "Epoch 120/300\n",
      "3886/3886 [==============================] - 1s 265us/step - loss: 11583.0233 - acc: 0.7694 - val_loss: 1591.2237 - val_acc: 0.7111\n",
      "Epoch 121/300\n",
      "3886/3886 [==============================] - 1s 249us/step - loss: 5884.3049 - acc: 0.7669 - val_loss: 2381.6285 - val_acc: 0.7167\n",
      "Epoch 122/300\n",
      "3886/3886 [==============================] - 1s 239us/step - loss: 5562.3619 - acc: 0.7692 - val_loss: 2708.2813 - val_acc: 0.7111\n",
      "Epoch 123/300\n",
      "3886/3886 [==============================] - 1s 235us/step - loss: 69130.2764 - acc: 0.7545 - val_loss: 3537.4681 - val_acc: 0.7389\n",
      "Epoch 124/300\n",
      "3886/3886 [==============================] - 1s 242us/step - loss: 58698.9674 - acc: 0.6806 - val_loss: 3727.6085 - val_acc: 0.7278\n",
      "Epoch 125/300\n",
      "3886/3886 [==============================] - 1s 241us/step - loss: 16429.8234 - acc: 0.7488 - val_loss: 3093.5946 - val_acc: 0.7056\n",
      "Epoch 126/300\n",
      "3886/3886 [==============================] - 1s 238us/step - loss: 25396.3040 - acc: 0.7622 - val_loss: 4782.8968 - val_acc: 0.6944\n",
      "Epoch 127/300\n",
      "3886/3886 [==============================] - 1s 244us/step - loss: 15325.7840 - acc: 0.7684 - val_loss: 3309.6245 - val_acc: 0.7167\n",
      "Epoch 128/300\n",
      "3886/3886 [==============================] - 1s 241us/step - loss: 295303.8172 - acc: 0.7553 - val_loss: 36800.3239 - val_acc: 0.7056\n",
      "Epoch 129/300\n",
      "3886/3886 [==============================] - 1s 243us/step - loss: 188143.6825 - acc: 0.7429 - val_loss: 5841.4914 - val_acc: 0.6833\n",
      "Epoch 130/300\n",
      "3886/3886 [==============================] - 1s 241us/step - loss: 56139.4626 - acc: 0.7267 - val_loss: 2787.5455 - val_acc: 0.6778\n",
      "Epoch 131/300\n",
      "3886/3886 [==============================] - 1s 248us/step - loss: 4180.3160 - acc: 0.7635 - val_loss: 2025.1345 - val_acc: 0.7222\n",
      "Epoch 132/300\n",
      "3886/3886 [==============================] - 1s 249us/step - loss: 58225.2398 - acc: 0.7540 - val_loss: 2302.3919 - val_acc: 0.7222\n",
      "Epoch 133/300\n",
      "3886/3886 [==============================] - 1s 246us/step - loss: 4321.2310 - acc: 0.7658 - val_loss: 1641.1586 - val_acc: 0.7278\n",
      "Epoch 134/300\n",
      "3886/3886 [==============================] - 1s 248us/step - loss: 14645.8367 - acc: 0.7643 - val_loss: 1810.5989 - val_acc: 0.7278\n",
      "Epoch 135/300\n",
      "3886/3886 [==============================] - 1s 240us/step - loss: 3687.5233 - acc: 0.7604 - val_loss: 4246.3804 - val_acc: 0.7278\n",
      "Epoch 136/300\n",
      "3886/3886 [==============================] - 1s 258us/step - loss: 28472.4365 - acc: 0.7615 - val_loss: 2286.3478 - val_acc: 0.7167\n",
      "Epoch 137/300\n",
      "3886/3886 [==============================] - 1s 252us/step - loss: 10553.2371 - acc: 0.7599 - val_loss: 3494.1483 - val_acc: 0.7278\n",
      "Epoch 138/300\n",
      "3886/3886 [==============================] - 1s 238us/step - loss: 151615.5171 - acc: 0.7542 - val_loss: 10577.8416 - val_acc: 0.7389\n",
      "Epoch 139/300\n",
      "3886/3886 [==============================] - 1s 239us/step - loss: 131224.4977 - acc: 0.7205 - val_loss: 4357.9461 - val_acc: 0.6944\n",
      "Epoch 140/300\n",
      "3886/3886 [==============================] - 1s 240us/step - loss: 178538.8754 - acc: 0.7383 - val_loss: 2821.9215 - val_acc: 0.6722\n",
      "Epoch 141/300\n",
      "3886/3886 [==============================] - 1s 239us/step - loss: 14364.6293 - acc: 0.7589 - val_loss: 1872.1420 - val_acc: 0.6889\n",
      "Epoch 142/300\n",
      "3886/3886 [==============================] - 1s 249us/step - loss: 30422.7688 - acc: 0.7609 - val_loss: 2505.6300 - val_acc: 0.6889\n",
      "Epoch 143/300\n",
      "3886/3886 [==============================] - 1s 247us/step - loss: 6609.4908 - acc: 0.7560 - val_loss: 2053.7384 - val_acc: 0.6944\n",
      "Epoch 144/300\n",
      "3886/3886 [==============================] - 1s 245us/step - loss: 38235.4408 - acc: 0.7594 - val_loss: 2197.4612 - val_acc: 0.7167\n",
      "Epoch 145/300\n",
      "3886/3886 [==============================] - 1s 261us/step - loss: 61459.6049 - acc: 0.7607 - val_loss: 3328.9109 - val_acc: 0.7056\n",
      "Epoch 146/300\n",
      "3886/3886 [==============================] - 1s 262us/step - loss: 12251.1268 - acc: 0.7555 - val_loss: 2827.2865 - val_acc: 0.7056\n",
      "Epoch 147/300\n",
      "3886/3886 [==============================] - 1s 253us/step - loss: 5899.1351 - acc: 0.7684 - val_loss: 1459.1616 - val_acc: 0.6944\n",
      "Epoch 148/300\n",
      "3886/3886 [==============================] - 1s 247us/step - loss: 15521.2547 - acc: 0.7684 - val_loss: 2178.9657 - val_acc: 0.7222\n",
      "Epoch 149/300\n",
      "3886/3886 [==============================] - 1s 242us/step - loss: 11115.1600 - acc: 0.7694 - val_loss: 1999.6746 - val_acc: 0.7111\n",
      "Epoch 150/300\n",
      "3886/3886 [==============================] - 1s 249us/step - loss: 210271.3893 - acc: 0.7501 - val_loss: 5542.3099 - val_acc: 0.7278\n",
      "Epoch 151/300\n",
      "3886/3886 [==============================] - 1s 242us/step - loss: 15348.9798 - acc: 0.7620 - val_loss: 3445.3566 - val_acc: 0.7167\n",
      "Epoch 152/300\n",
      "3886/3886 [==============================] - 1s 245us/step - loss: 31551.4765 - acc: 0.7620 - val_loss: 15376.0068 - val_acc: 0.7444\n",
      "Epoch 153/300\n",
      "3886/3886 [==============================] - 1s 245us/step - loss: 54335.9226 - acc: 0.7645 - val_loss: 17127.9564 - val_acc: 0.7389\n",
      "Epoch 154/300\n",
      "3886/3886 [==============================] - 1s 247us/step - loss: 93662.0421 - acc: 0.7576 - val_loss: 14028.8310 - val_acc: 0.7222\n",
      "Epoch 155/300\n",
      "3886/3886 [==============================] - 1s 244us/step - loss: 52911.5742 - acc: 0.7643 - val_loss: 2490.0966 - val_acc: 0.7222\n",
      "Epoch 156/300\n",
      "3886/3886 [==============================] - 1s 245us/step - loss: 2442.6037 - acc: 0.7741 - val_loss: 1779.5946 - val_acc: 0.7444\n",
      "Epoch 157/300\n",
      "3886/3886 [==============================] - 1s 250us/step - loss: 6651.4704 - acc: 0.7707 - val_loss: 4763.3172 - val_acc: 0.7333\n",
      "Epoch 158/300\n",
      "3886/3886 [==============================] - 1s 248us/step - loss: 153110.1936 - acc: 0.7707 - val_loss: 13352.2469 - val_acc: 0.6889\n",
      "Epoch 159/300\n",
      "3886/3886 [==============================] - 1s 243us/step - loss: 166572.9418 - acc: 0.7499 - val_loss: 2895.4626 - val_acc: 0.7222\n",
      "Epoch 160/300\n",
      "3886/3886 [==============================] - 1s 249us/step - loss: 5766.7399 - acc: 0.7751 - val_loss: 2350.0499 - val_acc: 0.7389\n",
      "Epoch 161/300\n",
      "3886/3886 [==============================] - 1s 246us/step - loss: 44848.4513 - acc: 0.7308 - val_loss: 5580.8030 - val_acc: 0.6889\n",
      "Epoch 162/300\n",
      "3886/3886 [==============================] - 1s 238us/step - loss: 8784.5371 - acc: 0.7293 - val_loss: 3600.1905 - val_acc: 0.7167\n",
      "Epoch 163/300\n",
      "3886/3886 [==============================] - 1s 234us/step - loss: 5264.7604 - acc: 0.7409 - val_loss: 3217.9395 - val_acc: 0.7222\n",
      "Epoch 164/300\n",
      "3886/3886 [==============================] - 1s 235us/step - loss: 118559.9285 - acc: 0.7519 - val_loss: 27173.8450 - val_acc: 0.6500\n",
      "Epoch 165/300\n",
      "3886/3886 [==============================] - 1s 246us/step - loss: 132747.6166 - acc: 0.7210 - val_loss: 27516.5306 - val_acc: 0.7222\n",
      "Epoch 166/300\n",
      "3886/3886 [==============================] - 1s 229us/step - loss: 192570.5396 - acc: 0.7349 - val_loss: 4935.6080 - val_acc: 0.5944\n",
      "Epoch 167/300\n",
      "3886/3886 [==============================] - 1s 234us/step - loss: 7848.8247 - acc: 0.6992 - val_loss: 2531.1452 - val_acc: 0.6389\n",
      "Epoch 168/300\n",
      "3886/3886 [==============================] - 1s 233us/step - loss: 3684.8509 - acc: 0.7301 - val_loss: 1439.3956 - val_acc: 0.6500\n",
      "Epoch 169/300\n",
      "3886/3886 [==============================] - 1s 232us/step - loss: 4432.8805 - acc: 0.7306 - val_loss: 6070.2958 - val_acc: 0.6778\n",
      "Epoch 170/300\n",
      "3886/3886 [==============================] - 1s 234us/step - loss: 4500.7109 - acc: 0.7573 - val_loss: 1543.9820 - val_acc: 0.6889\n",
      "Epoch 171/300\n",
      "3886/3886 [==============================] - 1s 234us/step - loss: 13476.7232 - acc: 0.7694 - val_loss: 1981.1935 - val_acc: 0.7167\n",
      "Epoch 172/300\n",
      "3886/3886 [==============================] - 1s 238us/step - loss: 72720.7866 - acc: 0.7229 - val_loss: 12071.8620 - val_acc: 0.6167\n",
      "Epoch 173/300\n",
      "3886/3886 [==============================] - 1s 240us/step - loss: 17438.6197 - acc: 0.7136 - val_loss: 2704.4073 - val_acc: 0.6667\n",
      "Epoch 174/300\n",
      "3886/3886 [==============================] - 1s 240us/step - loss: 167876.5240 - acc: 0.7223 - val_loss: 6526.2224 - val_acc: 0.6556\n",
      "Epoch 175/300\n",
      "3886/3886 [==============================] - 1s 257us/step - loss: 22595.2319 - acc: 0.7262 - val_loss: 9963.6861 - val_acc: 0.6667\n",
      "Epoch 176/300\n",
      "3886/3886 [==============================] - 1s 264us/step - loss: 93199.8853 - acc: 0.7488 - val_loss: 2941.9408 - val_acc: 0.6889\n",
      "Epoch 177/300\n",
      "3886/3886 [==============================] - 1s 256us/step - loss: 20237.2395 - acc: 0.7586 - val_loss: 2894.8181 - val_acc: 0.6889\n",
      "Epoch 178/300\n",
      "3886/3886 [==============================] - 1s 258us/step - loss: 35515.5002 - acc: 0.7499 - val_loss: 5549.3110 - val_acc: 0.7000\n",
      "Epoch 179/300\n",
      "3886/3886 [==============================] - 1s 251us/step - loss: 33383.5726 - acc: 0.7627 - val_loss: 3734.1969 - val_acc: 0.7111\n",
      "Epoch 180/300\n",
      "3886/3886 [==============================] - 1s 239us/step - loss: 4023.5139 - acc: 0.7648 - val_loss: 1026.0817 - val_acc: 0.7056\n",
      "Epoch 181/300\n",
      "3886/3886 [==============================] - 1s 236us/step - loss: 5296.0010 - acc: 0.7661 - val_loss: 5418.8388 - val_acc: 0.6667\n",
      "Epoch 182/300\n",
      "3886/3886 [==============================] - 1s 238us/step - loss: 58445.1106 - acc: 0.7566 - val_loss: 15942.8854 - val_acc: 0.6722\n",
      "Epoch 183/300\n",
      "3886/3886 [==============================] - 1s 239us/step - loss: 68470.4138 - acc: 0.7339 - val_loss: 11229.5794 - val_acc: 0.7278\n",
      "Epoch 184/300\n",
      "3886/3886 [==============================] - 1s 238us/step - loss: 134383.2735 - acc: 0.7545 - val_loss: 3296.9212 - val_acc: 0.7333\n",
      "Epoch 185/300\n",
      "3886/3886 [==============================] - 1s 237us/step - loss: 18798.2877 - acc: 0.7671 - val_loss: 1958.8112 - val_acc: 0.7389\n",
      "Epoch 186/300\n",
      "3886/3886 [==============================] - 1s 249us/step - loss: 3168.8078 - acc: 0.7674 - val_loss: 1470.3548 - val_acc: 0.7389\n",
      "Epoch 187/300\n",
      "3886/3886 [==============================] - 1s 246us/step - loss: 14308.7218 - acc: 0.7707 - val_loss: 2531.3453 - val_acc: 0.7444\n",
      "Epoch 188/300\n",
      "3886/3886 [==============================] - 1s 241us/step - loss: 57181.9016 - acc: 0.7712 - val_loss: 10249.5396 - val_acc: 0.7333\n",
      "Epoch 189/300\n",
      "3886/3886 [==============================] - 1s 244us/step - loss: 273604.5789 - acc: 0.7512 - val_loss: 3730.1998 - val_acc: 0.7389\n",
      "Epoch 190/300\n",
      "3886/3886 [==============================] - 1s 248us/step - loss: 5228.4545 - acc: 0.7519 - val_loss: 2900.6216 - val_acc: 0.7556\n",
      "Epoch 191/300\n",
      "3886/3886 [==============================] - 1s 249us/step - loss: 4685.1182 - acc: 0.7630 - val_loss: 1925.9921 - val_acc: 0.7722\n",
      "Epoch 192/300\n",
      "3886/3886 [==============================] - 1s 244us/step - loss: 6228.0812 - acc: 0.7735 - val_loss: 2199.6525 - val_acc: 0.7778\n",
      "Epoch 193/300\n",
      "3886/3886 [==============================] - 1s 243us/step - loss: 8998.9418 - acc: 0.7828 - val_loss: 2402.3613 - val_acc: 0.7778\n",
      "Epoch 194/300\n",
      "3886/3886 [==============================] - 1s 244us/step - loss: 52755.7611 - acc: 0.7828 - val_loss: 11309.6121 - val_acc: 0.7222\n",
      "Epoch 195/300\n",
      "3886/3886 [==============================] - 1s 240us/step - loss: 37214.5118 - acc: 0.7424 - val_loss: 2877.2961 - val_acc: 0.6278\n",
      "Epoch 196/300\n",
      "3886/3886 [==============================] - 1s 239us/step - loss: 53945.1430 - acc: 0.7597 - val_loss: 3818.0208 - val_acc: 0.7278\n",
      "Epoch 197/300\n",
      "3886/3886 [==============================] - 1s 248us/step - loss: 66574.4597 - acc: 0.7661 - val_loss: 3146.4055 - val_acc: 0.7500\n",
      "Epoch 198/300\n",
      "3886/3886 [==============================] - 1s 239us/step - loss: 6462.5428 - acc: 0.7769 - val_loss: 1696.2308 - val_acc: 0.7722\n",
      "Epoch 199/300\n",
      "3886/3886 [==============================] - 1s 241us/step - loss: 8219.4053 - acc: 0.7900 - val_loss: 4647.7722 - val_acc: 0.7722\n",
      "Epoch 200/300\n",
      "3886/3886 [==============================] - 1s 240us/step - loss: 140366.6188 - acc: 0.7414 - val_loss: 7319.7070 - val_acc: 0.7278\n",
      "Epoch 201/300\n",
      "3886/3886 [==============================] - 1s 249us/step - loss: 8832.7062 - acc: 0.7509 - val_loss: 2448.8749 - val_acc: 0.7278\n",
      "Epoch 202/300\n",
      "3886/3886 [==============================] - 1s 242us/step - loss: 34871.1023 - acc: 0.7720 - val_loss: 2749.5263 - val_acc: 0.7556\n",
      "Epoch 203/300\n",
      "3886/3886 [==============================] - 1s 246us/step - loss: 115546.9128 - acc: 0.7717 - val_loss: 3008.5569 - val_acc: 0.7500\n",
      "Epoch 204/300\n",
      "3886/3886 [==============================] - 1s 247us/step - loss: 22351.9993 - acc: 0.7771 - val_loss: 6540.9738 - val_acc: 0.7333\n",
      "Epoch 205/300\n",
      "3886/3886 [==============================] - 1s 251us/step - loss: 7644.7336 - acc: 0.7800 - val_loss: 6226.9753 - val_acc: 0.7222\n",
      "Epoch 206/300\n",
      "3886/3886 [==============================] - 1s 251us/step - loss: 46182.4149 - acc: 0.7756 - val_loss: 6117.1535 - val_acc: 0.7611\n",
      "Epoch 207/300\n",
      "3886/3886 [==============================] - 1s 246us/step - loss: 47640.8136 - acc: 0.7707 - val_loss: 2300.7936 - val_acc: 0.7278\n",
      "Epoch 208/300\n",
      "3886/3886 [==============================] - 1s 244us/step - loss: 6266.5859 - acc: 0.7766 - val_loss: 2503.2895 - val_acc: 0.7444\n",
      "Epoch 209/300\n",
      "3886/3886 [==============================] - 1s 248us/step - loss: 10317.8611 - acc: 0.7841 - val_loss: 1922.2448 - val_acc: 0.7444\n",
      "Epoch 210/300\n",
      "3886/3886 [==============================] - 1s 258us/step - loss: 7708.7608 - acc: 0.7831 - val_loss: 8841.2472 - val_acc: 0.7444\n",
      "Epoch 211/300\n",
      "3886/3886 [==============================] - 1s 252us/step - loss: 4640.9760 - acc: 0.7905 - val_loss: 2785.7988 - val_acc: 0.7222\n",
      "Epoch 212/300\n",
      "3886/3886 [==============================] - 1s 247us/step - loss: 35935.7129 - acc: 0.7877 - val_loss: 13302.5139 - val_acc: 0.7389\n",
      "Epoch 213/300\n",
      "3886/3886 [==============================] - 1s 242us/step - loss: 219941.6184 - acc: 0.7550 - val_loss: 3477.1931 - val_acc: 0.6889\n",
      "Epoch 214/300\n",
      "3886/3886 [==============================] - 1s 249us/step - loss: 5727.3704 - acc: 0.7836 - val_loss: 1331.1944 - val_acc: 0.7778\n",
      "Epoch 215/300\n",
      "3886/3886 [==============================] - 1s 271us/step - loss: 3499.0896 - acc: 0.7946 - val_loss: 1850.3848 - val_acc: 0.7667\n",
      "Epoch 216/300\n",
      "3886/3886 [==============================] - 1s 258us/step - loss: 21097.5388 - acc: 0.7887 - val_loss: 1392.7712 - val_acc: 0.7389\n",
      "Epoch 217/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3886/3886 [==============================] - 1s 253us/step - loss: 14071.3319 - acc: 0.7774 - val_loss: 1566.1416 - val_acc: 0.7444\n",
      "Epoch 218/300\n",
      "3886/3886 [==============================] - 1s 256us/step - loss: 6017.5159 - acc: 0.7823 - val_loss: 1221.4769 - val_acc: 0.7444\n",
      "Epoch 219/300\n",
      "3886/3886 [==============================] - 1s 254us/step - loss: 26519.3363 - acc: 0.7854 - val_loss: 4397.5428 - val_acc: 0.7722\n",
      "Epoch 220/300\n",
      "3886/3886 [==============================] - 1s 255us/step - loss: 104148.6067 - acc: 0.7689 - val_loss: 12179.5411 - val_acc: 0.6611\n",
      "Epoch 221/300\n",
      "3886/3886 [==============================] - 1s 259us/step - loss: 203727.0269 - acc: 0.6897 - val_loss: 6208.4625 - val_acc: 0.6667\n",
      "Epoch 222/300\n",
      "3886/3886 [==============================] - 1s 247us/step - loss: 9444.3064 - acc: 0.7298 - val_loss: 2395.1280 - val_acc: 0.6778\n",
      "Epoch 223/300\n",
      "3886/3886 [==============================] - 1s 245us/step - loss: 8219.4971 - acc: 0.7545 - val_loss: 3244.9531 - val_acc: 0.7500\n",
      "Epoch 224/300\n",
      "3886/3886 [==============================] - 1s 258us/step - loss: 5664.9019 - acc: 0.7640 - val_loss: 1682.8674 - val_acc: 0.7278\n",
      "Epoch 225/300\n",
      "3886/3886 [==============================] - 1s 247us/step - loss: 7965.5691 - acc: 0.7576 - val_loss: 2043.4904 - val_acc: 0.7111\n",
      "Epoch 226/300\n",
      "3886/3886 [==============================] - 1s 251us/step - loss: 9258.9365 - acc: 0.7445 - val_loss: 4571.3005 - val_acc: 0.7056\n",
      "Epoch 227/300\n",
      "3886/3886 [==============================] - 1s 245us/step - loss: 60239.3804 - acc: 0.7257 - val_loss: 7291.0574 - val_acc: 0.7389\n",
      "Epoch 228/300\n",
      "3886/3886 [==============================] - 1s 254us/step - loss: 98015.9136 - acc: 0.7571 - val_loss: 22817.4229 - val_acc: 0.7444\n",
      "Epoch 229/300\n",
      "3886/3886 [==============================] - 1s 261us/step - loss: 68726.6462 - acc: 0.7622 - val_loss: 1301.2840 - val_acc: 0.7222\n",
      "Epoch 230/300\n",
      "3886/3886 [==============================] - 1s 259us/step - loss: 7922.7834 - acc: 0.7723 - val_loss: 1702.4323 - val_acc: 0.7444\n",
      "Epoch 231/300\n",
      "3886/3886 [==============================] - 1s 258us/step - loss: 8081.2068 - acc: 0.7635 - val_loss: 5893.2131 - val_acc: 0.6833\n",
      "Epoch 232/300\n",
      "3886/3886 [==============================] - 1s 261us/step - loss: 34624.1159 - acc: 0.7146 - val_loss: 6410.7831 - val_acc: 0.6500\n",
      "Epoch 233/300\n",
      "3886/3886 [==============================] - 1s 278us/step - loss: 89217.8386 - acc: 0.7033 - val_loss: 4751.4946 - val_acc: 0.7111\n",
      "Epoch 234/300\n",
      "3886/3886 [==============================] - 1s 265us/step - loss: 20644.4228 - acc: 0.7679 - val_loss: 2369.1343 - val_acc: 0.7444\n",
      "Epoch 235/300\n",
      "3886/3886 [==============================] - 1s 268us/step - loss: 4445.5925 - acc: 0.7710 - val_loss: 1699.1999 - val_acc: 0.7389\n",
      "Epoch 236/300\n",
      "3886/3886 [==============================] - 1s 252us/step - loss: 2874.3820 - acc: 0.7880 - val_loss: 3246.2449 - val_acc: 0.7389\n",
      "Epoch 237/300\n",
      "3886/3886 [==============================] - 1s 246us/step - loss: 11941.0816 - acc: 0.7800 - val_loss: 7287.8882 - val_acc: 0.7167\n",
      "Epoch 238/300\n",
      "3886/3886 [==============================] - 1s 254us/step - loss: 239412.6303 - acc: 0.7373 - val_loss: 8462.7615 - val_acc: 0.6778\n",
      "Epoch 239/300\n",
      "3886/3886 [==============================] - 1s 251us/step - loss: 76857.1452 - acc: 0.6660 - val_loss: 3744.2994 - val_acc: 0.7167\n",
      "Epoch 240/300\n",
      "3886/3886 [==============================] - 1s 255us/step - loss: 4823.3967 - acc: 0.7663 - val_loss: 2989.8044 - val_acc: 0.7278\n",
      "Epoch 241/300\n",
      "3886/3886 [==============================] - 1s 268us/step - loss: 3577.9121 - acc: 0.7658 - val_loss: 1515.3048 - val_acc: 0.7500\n",
      "Epoch 242/300\n",
      "3886/3886 [==============================] - 1s 251us/step - loss: 3940.7506 - acc: 0.7640 - val_loss: 1457.5667 - val_acc: 0.7444\n",
      "Epoch 243/300\n",
      "3886/3886 [==============================] - 1s 247us/step - loss: 4910.4964 - acc: 0.7638 - val_loss: 1812.4529 - val_acc: 0.7444\n",
      "Epoch 244/300\n",
      "3886/3886 [==============================] - 1s 264us/step - loss: 38866.6551 - acc: 0.7123 - val_loss: 1481.0398 - val_acc: 0.7389\n",
      "Epoch 245/300\n",
      "3886/3886 [==============================] - 1s 278us/step - loss: 73016.8523 - acc: 0.7285 - val_loss: 1949.5234 - val_acc: 0.7167\n",
      "Epoch 246/300\n",
      "3886/3886 [==============================] - 1s 267us/step - loss: 3984.9172 - acc: 0.7416 - val_loss: 1315.6191 - val_acc: 0.7333\n",
      "Epoch 247/300\n",
      "3886/3886 [==============================] - 1s 256us/step - loss: 2479.8854 - acc: 0.7573 - val_loss: 1316.8493 - val_acc: 0.7444\n",
      "Epoch 248/300\n",
      "3886/3886 [==============================] - 1s 263us/step - loss: 7484.2842 - acc: 0.7620 - val_loss: 2900.4100 - val_acc: 0.7333\n",
      "Epoch 249/300\n",
      "3886/3886 [==============================] - 1s 272us/step - loss: 72546.2999 - acc: 0.7560 - val_loss: 5908.0682 - val_acc: 0.7389\n",
      "Epoch 250/300\n",
      "3886/3886 [==============================] - 1s 263us/step - loss: 4337.8176 - acc: 0.7833 - val_loss: 1814.4493 - val_acc: 0.7444\n",
      "Epoch 251/300\n",
      "3886/3886 [==============================] - 1s 277us/step - loss: 3521.9871 - acc: 0.7851 - val_loss: 1467.1676 - val_acc: 0.7556\n",
      "Epoch 252/300\n",
      "3886/3886 [==============================] - 1s 252us/step - loss: 60308.9936 - acc: 0.7702 - val_loss: 31411.3411 - val_acc: 0.7056\n",
      "Epoch 253/300\n",
      "3886/3886 [==============================] - 1s 260us/step - loss: 142560.2968 - acc: 0.7054 - val_loss: 4440.3599 - val_acc: 0.7167\n",
      "Epoch 254/300\n",
      "3886/3886 [==============================] - 1s 260us/step - loss: 59852.3645 - acc: 0.7367 - val_loss: 2877.1400 - val_acc: 0.7056\n",
      "Epoch 255/300\n",
      "3886/3886 [==============================] - 1s 267us/step - loss: 20971.6756 - acc: 0.7540 - val_loss: 1755.0908 - val_acc: 0.7389\n",
      "Epoch 256/300\n",
      "3886/3886 [==============================] - 1s 263us/step - loss: 3072.0604 - acc: 0.7715 - val_loss: 4339.7733 - val_acc: 0.7389\n",
      "Epoch 257/300\n",
      "3886/3886 [==============================] - 1s 269us/step - loss: 5051.5064 - acc: 0.7779 - val_loss: 1411.2248 - val_acc: 0.7556\n",
      "Epoch 258/300\n",
      "3886/3886 [==============================] - 1s 261us/step - loss: 6051.2315 - acc: 0.7800 - val_loss: 1536.2935 - val_acc: 0.7444\n",
      "Epoch 259/300\n",
      "3886/3886 [==============================] - 1s 262us/step - loss: 9945.3719 - acc: 0.7774 - val_loss: 2721.9671 - val_acc: 0.7333\n",
      "Epoch 260/300\n",
      "3886/3886 [==============================] - 1s 264us/step - loss: 144516.6635 - acc: 0.7427 - val_loss: 12962.9274 - val_acc: 0.7222\n",
      "Epoch 261/300\n",
      "3886/3886 [==============================] - 1s 261us/step - loss: 6932.9698 - acc: 0.7658 - val_loss: 2365.7143 - val_acc: 0.7222\n",
      "Epoch 262/300\n",
      "3886/3886 [==============================] - 1s 268us/step - loss: 8417.6548 - acc: 0.7707 - val_loss: 3839.9683 - val_acc: 0.7278\n",
      "Epoch 263/300\n",
      "3886/3886 [==============================] - 1s 267us/step - loss: 31824.7606 - acc: 0.7730 - val_loss: 1428.1155 - val_acc: 0.7389\n",
      "Epoch 264/300\n",
      "3886/3886 [==============================] - 1s 274us/step - loss: 84714.1525 - acc: 0.7779 - val_loss: 19182.6464 - val_acc: 0.6278\n",
      "Epoch 265/300\n",
      "3886/3886 [==============================] - 1s 250us/step - loss: 105970.6727 - acc: 0.7110 - val_loss: 7495.6657 - val_acc: 0.7556\n",
      "Epoch 266/300\n",
      "3886/3886 [==============================] - 1s 266us/step - loss: 25455.7371 - acc: 0.7432 - val_loss: 3415.9575 - val_acc: 0.7278\n",
      "Epoch 267/300\n",
      "3886/3886 [==============================] - 1s 266us/step - loss: 4327.7285 - acc: 0.7759 - val_loss: 1335.9696 - val_acc: 0.7611\n",
      "Epoch 268/300\n",
      "3886/3886 [==============================] - 1s 261us/step - loss: 2838.9101 - acc: 0.7892 - val_loss: 1126.4821 - val_acc: 0.7944\n",
      "Epoch 269/300\n",
      "3886/3886 [==============================] - 1s 264us/step - loss: 2267.9778 - acc: 0.7967 - val_loss: 1448.6270 - val_acc: 0.7833\n",
      "Epoch 270/300\n",
      "3886/3886 [==============================] - 1s 274us/step - loss: 59498.7058 - acc: 0.7715 - val_loss: 12581.3668 - val_acc: 0.7222\n",
      "Epoch 271/300\n",
      "3886/3886 [==============================] - 1s 260us/step - loss: 527738.5618 - acc: 0.6938 - val_loss: 22799.1150 - val_acc: 0.6000\n",
      "Epoch 272/300\n",
      "3886/3886 [==============================] - 1s 265us/step - loss: 26302.8366 - acc: 0.7566 - val_loss: 10036.0594 - val_acc: 0.7222\n",
      "Epoch 273/300\n",
      "3886/3886 [==============================] - 1s 236us/step - loss: 14020.2367 - acc: 0.7697 - val_loss: 4688.7504 - val_acc: 0.7500\n",
      "Epoch 274/300\n",
      "3886/3886 [==============================] - 1s 269us/step - loss: 7106.4447 - acc: 0.7661 - val_loss: 3713.3547 - val_acc: 0.7444\n",
      "Epoch 275/300\n",
      "3886/3886 [==============================] - 1s 267us/step - loss: 18822.9824 - acc: 0.7620 - val_loss: 2841.8315 - val_acc: 0.7556\n",
      "Epoch 276/300\n",
      "3886/3886 [==============================] - 1s 259us/step - loss: 4796.4481 - acc: 0.7560 - val_loss: 2748.1123 - val_acc: 0.7444\n",
      "Epoch 277/300\n",
      "3886/3886 [==============================] - 1s 257us/step - loss: 37220.3147 - acc: 0.7578 - val_loss: 2039.4056 - val_acc: 0.7444\n",
      "Epoch 278/300\n",
      "3886/3886 [==============================] - 1s 253us/step - loss: 8319.2364 - acc: 0.7622 - val_loss: 1889.3760 - val_acc: 0.7444\n",
      "Epoch 279/300\n",
      "3886/3886 [==============================] - 1s 252us/step - loss: 3357.7217 - acc: 0.7640 - val_loss: 1885.9588 - val_acc: 0.7444\n",
      "Epoch 280/300\n",
      "3886/3886 [==============================] - 1s 254us/step - loss: 6112.2280 - acc: 0.7712 - val_loss: 2871.8081 - val_acc: 0.7500\n",
      "Epoch 281/300\n",
      "3886/3886 [==============================] - 1s 246us/step - loss: 83989.7001 - acc: 0.7247 - val_loss: 9696.2526 - val_acc: 0.7000\n",
      "Epoch 282/300\n",
      "3886/3886 [==============================] - 1s 248us/step - loss: 195942.9953 - acc: 0.7097 - val_loss: 4009.7156 - val_acc: 0.7111\n",
      "Epoch 283/300\n",
      "3886/3886 [==============================] - 1s 254us/step - loss: 22211.7873 - acc: 0.7524 - val_loss: 2542.8581 - val_acc: 0.7389\n",
      "Epoch 284/300\n",
      "3886/3886 [==============================] - 1s 250us/step - loss: 42439.0191 - acc: 0.7571 - val_loss: 2932.7131 - val_acc: 0.7333\n",
      "Epoch 285/300\n",
      "3886/3886 [==============================] - 1s 259us/step - loss: 44169.1447 - acc: 0.7684 - val_loss: 1528.1885 - val_acc: 0.7333\n",
      "Epoch 286/300\n",
      "3886/3886 [==============================] - 1s 254us/step - loss: 6370.7878 - acc: 0.7728 - val_loss: 1786.1436 - val_acc: 0.7278\n",
      "Epoch 287/300\n",
      "3886/3886 [==============================] - 1s 249us/step - loss: 29915.3367 - acc: 0.7787 - val_loss: 13558.3497 - val_acc: 0.7333\n",
      "Epoch 288/300\n",
      "3886/3886 [==============================] - 1s 239us/step - loss: 67580.8114 - acc: 0.7748 - val_loss: 2006.1009 - val_acc: 0.7556\n",
      "Epoch 289/300\n",
      "3886/3886 [==============================] - ETA: 0s - loss: 3818.3838 - acc: 0.78 - 1s 249us/step - loss: 3795.2285 - acc: 0.7813 - val_loss: 2291.7509 - val_acc: 0.7611\n",
      "Epoch 290/300\n",
      "3886/3886 [==============================] - 1s 242us/step - loss: 148654.3583 - acc: 0.7532 - val_loss: 4906.4423 - val_acc: 0.7500\n",
      "Epoch 291/300\n",
      "3886/3886 [==============================] - 1s 240us/step - loss: 5813.3045 - acc: 0.7802 - val_loss: 2837.7649 - val_acc: 0.7778\n",
      "Epoch 292/300\n",
      "3886/3886 [==============================] - 1s 241us/step - loss: 10274.5856 - acc: 0.7820 - val_loss: 2524.5788 - val_acc: 0.7722\n",
      "Epoch 293/300\n",
      "3886/3886 [==============================] - 1s 241us/step - loss: 3486.8769 - acc: 0.7874 - val_loss: 1535.3388 - val_acc: 0.7556\n",
      "Epoch 294/300\n",
      "3886/3886 [==============================] - 1s 239us/step - loss: 2390.3428 - acc: 0.7869 - val_loss: 1359.7341 - val_acc: 0.7722\n",
      "Epoch 295/300\n",
      "3886/3886 [==============================] - 1s 240us/step - loss: 12131.9827 - acc: 0.7872 - val_loss: 1302.7588 - val_acc: 0.7667\n",
      "Epoch 296/300\n",
      "3886/3886 [==============================] - 1s 239us/step - loss: 15693.5678 - acc: 0.7882 - val_loss: 3338.0081 - val_acc: 0.7833\n",
      "Epoch 297/300\n",
      "3886/3886 [==============================] - 1s 241us/step - loss: 114465.1855 - acc: 0.7571 - val_loss: 7320.0684 - val_acc: 0.7333\n",
      "Epoch 298/300\n",
      "3886/3886 [==============================] - 1s 241us/step - loss: 147669.3571 - acc: 0.7437 - val_loss: 5549.0635 - val_acc: 0.7111\n",
      "Epoch 299/300\n",
      "3886/3886 [==============================] - 1s 247us/step - loss: 14147.6362 - acc: 0.7411 - val_loss: 1819.4688 - val_acc: 0.7333\n",
      "Epoch 300/300\n",
      "3886/3886 [==============================] - 1s 250us/step - loss: 2360.9503 - acc: 0.7548 - val_loss: 1568.9945 - val_acc: 0.7556\n",
      "180/180 [==============================] - 0s 54us/step\n",
      "\n",
      "acc: 75.56%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "\n",
    "numpy.random.seed(7)\n",
    "\n",
    "#dataset = numpy.loadtxt(\"data15min.csv\", delimiter=\",\", skiprows=1, usecols=range(2,27))\n",
    "\n",
    "#X = Y = dataset\n",
    "\n",
    "dataset2 = pd.read_csv(\"louis.csv\")\n",
    "dataset = pd.read_csv(\"data15min.csv\")\n",
    "dataset = dataset[0:len(dataset2)]\n",
    "dataset = pd.concat([dataset, dataset2])\n",
    "dataset = dataset.drop([\"n\", \"time\"], axis=1)\n",
    "dataset = shuffle(dataset)\n",
    "X = Y = dataset.sample(frac=0.8)\n",
    "X_test = Y_test = dataset.drop(X.index)\n",
    "activation = 'linear'\n",
    "\n",
    "X = Y = X.as_matrix()\n",
    "X_test = Y_test = X_test.as_matrix()\n",
    "activation = 'linear'\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50\t, activation=\"relu\", input_dim=25))\n",
    "model.add(Dense(15\t, activation=\"linear\"))\n",
    "model.add(Dense(5\t, activation=activation))\n",
    "model.add(Dense(15\t, activation=\"linear\"))\n",
    "model.add(Dense(50\t, activation=\"relu\"))\n",
    "model.add(Dense(25\t, activation=activation))\n",
    "model.summary()\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, Y, epochs=300, batch_size=10,  validation_data=(X_test, Y_test))\n",
    "\n",
    "scores = model.evaluate(X_test, Y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "model.save(\"model.h5\")\n",
    "\n",
    "model.save_weights('my_model_weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_103 (Dense)            (None, 170)               4420      \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 140)               23940     \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 5)                 705       \n",
      "=================================================================\n",
      "Total params: 29,065\n",
      "Trainable params: 29,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_103_input (InputLayer) (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 170)               4420      \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 140)               23940     \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 5)                 705       \n",
      "=================================================================\n",
      "Total params: 29,065\n",
      "Trainable params: 29,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import load_model\n",
    "from keras import *\n",
    "import numpy as np\n",
    "\n",
    "dataset = np.loadtxt(\"data15min.csv\", delimiter=\",\", skiprows=1, usecols=range(2,27))\n",
    "\n",
    "X = Y = dataset\n",
    "model = load_model(\"model.h5\")\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "\n",
    "model.outputs = [model.layers[-1].output]\n",
    "model.layers[-1].outbound_nodes = []\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "inp = model.input\n",
    "out = model.layers[-1].output\n",
    "model2 = Model(inp, out)\n",
    "model2.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model2.summary()\n",
    "\n",
    "output = model2.predict(X)\n",
    "#model2.save(\"model2.h5\")\n",
    "#np.savetxt(\"output.csv\", output, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19_input (InputLayer)  (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 150)               3900      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 120)               18120     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 5)                 605       \n",
      "=================================================================\n",
      "Total params: 22,625\n",
      "Trainable params: 22,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import load_model\n",
    "from keras import *\n",
    "import numpy as np\n",
    "model3 = load_model(\"model2.h5\")\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2429/2429 [==============================] - 1s 544us/step\n",
      "[2719.532635229809, 0.6908192668936781]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_353 (Dense)            (None, 50)                1300      \n",
      "_________________________________________________________________\n",
      "dense_354 (Dense)            (None, 15)                765       \n",
      "_________________________________________________________________\n",
      "dense_355 (Dense)            (None, 5)                 80        \n",
      "=================================================================\n",
      "Total params: 2,145\n",
      "Trainable params: 2,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[26  8  2  1  0  9  3  1  1  0 11  9  2  2  0 11  4  2  2  1 25  8  3  1\n",
      "  0]\n",
      "[ 19.41621   -23.697723   -3.3560827 -16.857092  -35.094078 ]\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"louis.csv\")\n",
    "dataset = dataset.drop([\"n\", \"time\"], axis=1)\n",
    "X = Y = dataset\n",
    "\n",
    "activation = 'linear'\n",
    "\n",
    "X = Y = X.as_matrix()\n",
    "activation = 'linear'\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50\t, activation=\"relu\", input_dim=25))\n",
    "model.add(Dense(15\t, activation=\"linear\"))\n",
    "model.add(Dense(5\t, activation=activation))\n",
    "model.add(Dense(15\t, activation=\"linear\"))\n",
    "model.add(Dense(50\t, activation=\"relu\"))\n",
    "model.add(Dense(25\t, activation=activation))\n",
    "model.load_weights(\"my_model_weights.h5\")\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.evaluate(X, Y))\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "\n",
    "model.outputs = [model.layers[-1].output]\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "output = model.predict(X)\n",
    "print(X[0])\n",
    "print(output[0])\n",
    "model.save(\"model_correct.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
